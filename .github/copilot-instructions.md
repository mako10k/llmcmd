# llmcmd - LLM Command Line Tool

## QAå¤§åŸå‰‡ï¼ˆå“è³ªä¿è¨¼åŸºæœ¬æ–¹é‡ï¼‰

### å››å¤§åŸå‰‡
1. **Silent Fallback å®Ÿè£…ã®å³ç¦**
   - ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ç„¡é€šçŸ¥å‡¦ç†ç¶™ç¶šã‚’ç¦æ­¢
   - Fail-FirståŸå‰‡ã®å¾¹åº•ï¼ˆerrcheckã€staticcheckã§æ¤œå‡ºï¼‰

2. **åŒç­‰æ©Ÿèƒ½ã®é‡è¤‡å®Ÿè£…ã®å³ç¦**
   - 42ã‚³ãƒãƒ³ãƒ‰é‡è¤‡å•é¡Œã®æ ¹æœ¬è§£æ±º
   - jscpdã€gocriticã§æ¤œå‡ºã€å…±é€šåŒ–ãƒ»interfaceåŒ–æ¨é€²

3. **ãƒ•ã‚¡ã‚¤ãƒ«é•·å¤§åŒ–ã®å³ç¦**
   - 1ãƒ•ã‚¡ã‚¤ãƒ«1000è¡Œæœªæº€ã€1é–¢æ•°50è¡Œæœªæº€ï¼ˆæ¨å¥¨ï¼‰
   - é©åˆ‡ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†å‰²ãƒ»è²¬ä»»åˆ†ç•Œã®å®Ÿæ–½

4. **è¤‡é›‘åº¦ã®å¢—å¤§ã®å³ç¦**
   - å¾ªç’°çš„è¤‡é›‘åº¦10æœªæº€ï¼ˆgocycloã§æ¸¬å®šï¼‰
   - é–¢æ•°åˆ†å‰²ãƒ»Early Returnãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç°¡ç´ åŒ–

### è‡ªå‹•ãƒã‚§ãƒƒã‚¯ä½“åˆ¶
- **pre-commitãƒ•ãƒƒã‚¯**: errcheckã€jscpdã€gocycloã€file-size-check
- **CIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: GitHub Actionsé€£æºã€PRæ™‚å¿…é ˆãƒã‚§ãƒƒã‚¯
- **é™çš„è§£æãƒ„ãƒ¼ãƒ«**: gosecã€golangci-lintã€staticcheck

### æ®µéšçš„å°å…¥
- Sprint 1: ãƒ„ãƒ¼ãƒ«å°å…¥ãƒ»ç¾çŠ¶æŠŠæ¡
- Sprint 2: é‡å¤§é•åä¿®æ­£
- Sprint 3ä»¥é™: åŸºæº–å³æ ¼åŒ–ãƒ»ç›£æŸ»è‡ªå‹•åŒ–

### ğŸ›¡ï¸ Premium Request Protection Protocol
**ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆæˆæœç‰©ä¿è­·åŸå‰‡**

#### ä¿è­·å¯¾è±¡ï¼ˆä¾¡å€¤ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼‰
- **ãƒ‡ãƒãƒƒã‚°ãƒ»ç¾å®Ÿèå’Œæ¸ˆã¿å®Ÿè£…**: ã‚¨ãƒ©ãƒ¼è§£æ±ºã€ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã€å®Ÿå‹•ä½œç¢ºèªã‚’çµŒãŸæˆæœç‰©
- **æ™‚é–“æŠ•å…¥æ¸ˆã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**: 30åˆ†ä»¥ä¸Šã®å®Ÿè£…ãƒ»èª¿æ•´ãƒ»æ¤œè¨¼ã‚’è¦ã—ãŸå®Ÿè£…
- **ç¾å®ŸçŸ¥è­˜çµ±åˆæ¸ˆã¿**: ç´”ç²‹ç”Ÿæˆã‚’è¶…ãˆã¦å®Ÿç’°å¢ƒã§ã®å•é¡Œè§£æ±ºã‚’å«ã‚€æˆæœç‰©

#### ä¿è­·å¯¾è±¡å¤–ï¼ˆå†ç”Ÿæˆå¯èƒ½ï¼‰
- **ç´”ç²‹ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ**: ãƒ‡ãƒãƒƒã‚°ãƒ»æ¤œè¨¼ãªã—ã®æ¨™æº–çš„ç”Ÿæˆç‰©
- **è©¦ä½œãƒ»å®Ÿé¨“æ®µéš**: æœªæ¤œè¨¼ãƒ»æœªçµ±åˆã®å®Ÿè£…
- **æ˜ã‚‰ã‹ã«å†åˆ©ç”¨ä¸å¯**: ä¸€æ™‚çš„ãƒ»ç‰¹å®šçŠ¶æ³é™å®šã®æˆæœç‰©

#### ä¿è­·æ‰‹é †
1. **WIPãƒ–ãƒ©ãƒ³ãƒä¿å­˜**: `feature/priority[N]-[feature-name]` å½¢å¼ã§ãƒ–ãƒ©ãƒ³ãƒä½œæˆ
2. **è©³ç´°ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: å®Ÿè£…å†…å®¹ã€æŠ€è¡“åˆ¤æ–­ç†ç”±ã€ä¿å­˜æ ¹æ‹ ã‚’æ˜è¨˜
3. **ãƒªãƒ¢ãƒ¼ãƒˆãƒ—ãƒƒã‚·ãƒ¥**: GitHubä¸Šã§ã®æ°¸ç¶šä¿å­˜ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
4. **ä»£æ›¿æ¤œè¨**: å‰Šé™¤ã§ã¯ãªãä»£æ›¿å®Ÿè£…ãƒ»æ”¹è‰¯ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å„ªå…ˆ

#### åˆ¤æ–­åŸºæº–
- **ä¿å­˜åŸºæº–**: ãƒ‡ãƒãƒƒã‚°ãƒ»ç¾å®Ÿèå’Œãƒ—ãƒ­ã‚»ã‚¹ã‚’çµŒãŸè²´é‡ãªçŸ¥è­˜
- **ä¿å­˜åˆ¤æ–­**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å”è­°ã«ã‚ˆã‚‹ä¾¡å€¤ãƒ™ãƒ¼ã‚¹åˆ¤å®š
- **æ–‡æ›¸åŒ–**: ä¿å­˜ç†ç”±ãƒ»ç¾å®Ÿèå’Œãƒ—ãƒ­ã‚»ã‚¹ã‚’å¿…ãšè¨˜éŒ²

#### ä¾‹å¤–å‡¦ç†
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯**: æ˜ç¢ºãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡ŒãŒã‚ã‚‹å ´åˆã®ã¿å‰Šé™¤æ¤œè¨
- **æ³•çš„å•é¡Œ**: ãƒ©ã‚¤ã‚»ãƒ³ã‚¹é•åç­‰ã®æ³•çš„å•é¡ŒãŒã‚ã‚‹å ´åˆã®ã¿å‰Šé™¤æ¤œè¨
- **äº‹å‰ç›¸è«‡**: å‰Šé™¤ãŒå¿…è¦ãªå ´åˆã¯å¿…ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨äº‹å‰ç›¸è«‡

**åŸå‰‡**: "Preserve debugged reality-integrated work, not just patterns" - ãƒ‡ãƒãƒƒã‚°ãƒ»ç¾å®Ÿèå’Œã—ãŸæˆæœç‰©ã¯è²´é‡ãªçŸ¥è­˜ã¨ã—ã¦ä¿è­·ã€ç´”ç²‹ç”Ÿæˆç‰©ã¯å†ç”Ÿæˆå¯èƒ½

## Project Team (MCP LLM Generator Contexts)

### Core Team Context IDs
- **PersonalityManager**: `context-mdtvqb20-opmlhg` - äººæ ¼ç®¡ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆãƒãƒ¼ãƒ æ§‹æˆãƒ»å½¹å‰²è¨­è¨ˆï¼‰
- **ProductOwner**: `context-mdtvs82o-1ekq4d` - è£½å“è²¬ä»»è€…ï¼ˆãƒ“ã‚¸ãƒ§ãƒ³ç­–å®šãƒ»è¦ä»¶å„ªå…ˆåº¦æ±ºå®šï¼‰
- **ScrumMaster**: `context-mdtvso8o-bljl6h` - ãƒ—ãƒ­ã‚»ã‚¹ä¿ƒé€²è€…ï¼ˆéšœå®³é™¤å»ãƒ»ãƒãƒ¼ãƒ ç”Ÿç”£æ€§å‘ä¸Šï¼‰
- **TechnicalLead**: `context-mdtvu1aq-e07dnk` - æŠ€è¡“è²¬ä»»è€…ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆãƒ»æŠ€è¡“æ„æ€æ±ºå®šï¼‰

### Quality & Process Team Context IDs
- **QAEngineer**: `context-mdtvvnz6-exfq23` - å“è³ªä¿è¨¼ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼ˆãƒ†ã‚¹ãƒˆæˆ¦ç•¥ãƒ»å“è³ªåŸºæº–ãƒ»è‡ªå‹•åŒ–æ¨é€²ï¼‰
- **GitWorkflowSpecialist**: `context-mdtvvzyi-95b60o` - Gitç®¡ç†ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆï¼ˆãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥ãƒ»ãƒªãƒªãƒ¼ã‚¹ç®¡ç†ï¼‰
- **PragmaticAdvisor**: `context-mdtvwj2v-h5ld7v` - ç¾å®Ÿçš„èª¿æ•´å½¹ï¼ˆç‡ç›´ãªæ„è¦‹ãƒ»å®Ÿè£…å¯èƒ½æ€§ãƒ»ãƒªã‚¹ã‚¯è©•ä¾¡ï¼‰

## Project Overview

`llmcmd` is a command-line tool that enables Large Language Models (LLMs) to execute tasks using the OpenAI ChatCompletion API. The tool provides LLMs with secure, built-in functions for file operations and text processing without external command execution.

## Development Goals

1. **Security First**: All operations are sandboxed with built-in commands only
2. **Simplicity**: Clean, maintainable code using compiled languages
3. **Efficiency**: Optimized API usage with proper cost controls
4. **Extensibility**: Future-ready architecture for additional features
5. **Reliability**: Fail-First principle with contract programming

## Development Principles

### ğŸš¨ CRITICAL: Fail-First Principle
- **Immediate Termination**: When an error occurs, terminate the program immediately
- **No Error Hiding**: NEVER hide, suppress, or continue processing after errors
- **Clear Error Messages**: Always provide clear, actionable error messages
- **Early Detection**: Detect and report errors as early as possible

### Contract Programming
- **Preconditions**: Validate all input parameters and system state before execution
- **Postconditions**: Verify expected outcomes where applicable
- **Assertions**: Use explicit checks for critical assumptions
- **Documentation**: Document all contracts in code comments

### Error Handling Rules
1. **Fatal Errors**: Use `log.Fatal()` or `os.Exit(1)` for unrecoverable errors
2. **Expected Errors**: Return explicit error values, handle at appropriate level
3. **Validation**: Check all inputs, nil pointers, and boundary conditions
4. **No Silent Failures**: Every error path must be visible and actionable

### Fallback Guidelines
**Immediate Termination Required:**
- **User Input Errors**: Parse errors, invalid values, wrong formats â†’ User needs to fix
- **Configuration Errors**: Invalid settings, malformed files â†’ User needs to correct
- **Logic Violations**: Precondition failures, contract violations â†’ Programming error
- **Security Issues**: Authentication failures, permission denials â†’ Must not continue

**Limited Fallback Allowed (with consultation):**
- **Missing Optional Files**: Use defaults when file absence is normal behavior
- **Network Transient Errors**: Retry with clear limits, then terminate
- **Resource Constraints**: Temporary issues that may resolve with retry

**Rule**: When in doubt, terminate immediately. User errors should cause immediate failure with clear error messages.

### Code Quality Standards
- **Defensive Programming**: Assume inputs are invalid until proven otherwise
- **Explicit Error Paths**: Every function that can fail must return an error
- **Resource Cleanup**: Always use proper cleanup (defer, close, etc.)
- **Testing**: Test error conditions extensively, not just happy paths

## Core Constraints

### Language Policy
- **Codebase**: English only (code, comments, variable names)
- **Commands**: English only
- **Runtime**: Japanese input allowed for user instructions
- **Documentation**: English preferred, Japanese acceptable for implementation notes

### Security Requirements
- **No external command execution**: All text processing via built-in functions
- **File access limited**: Only specified input/output files
- **API call limits**: Configurable maximum calls per session (default: 50)
- **Quota management**: Weighted token tracking with limits
- **Memory limits**: 4KB read buffer, 10MB file size limits

## Architecture

### Technology Stack
- **Language**: Go 1.21+
- **API**: OpenAI ChatCompletion with Function Calling
- **Architecture**: Layered design with clean separation

### Core Components
1. **CLI Parser**: Command-line argument processing
2. **Configuration Manager**: Config file and environment variable handling
3. **OpenAI API Client**: ChatCompletion API communication
4. **Tool Execution Engine**: LLM tool call processing
5. **Built-in Commands**: Security-focused text processing functions

## Development Environment

### Required Tools
- **Shell Operations**: Use #mcp-shell-server for all terminal commands
- **Code Editor**: VS Code
- **Go Version**: 1.21 or higher

### âš ï¸ CRITICAL: Tool Selection Requirements

**ğŸš« DO NOT USE `run_in_terminal`** - This tool has known output reading bugs and cannot reliably capture command outputs, especially when they are truncated or large.

**âœ… ALWAYS USE `#mcp-shell-server`** - This is the ONLY approved method for executing shell commands in this project. It provides:
- Reliable output capture with truncation handling
- Background process management
- Complete output retrieval via `output_id`
- Proper error handling and status monitoring

**Violation of this rule will cause development failures and incomplete command outputs.**

### mcp-shell-server Usage Notes
- **Output Tracking**: When command output is truncated, use the returned `output_id` with `read_execution_output` to get complete results
- **Common Mistake**: Don't assume output is complete when truncated - always check for `output_truncated: true` and use `output_id` to retrieve full content
- **Background Processes**: Long-running commands automatically switch to background mode and provide `execution_id` for status monitoring

### Development Workflow
1. **ğŸ”¥ MANDATORY**: Use #mcp-shell-server for Git commands and build operations - NEVER use run_in_terminal
2. Incremental implementation and testing
3. Proper commit messages with clear change descriptions
4. Phase-based development approach

### Shell Command Execution Rules
- **Git Operations**: `#mcp-shell-server` only (git status, git commit, git push, etc.)
- **Build Commands**: `#mcp-shell-server` only (go build, go test, go mod, etc.)
- **File Operations**: `#mcp-shell-server` only (grep, find, ls, etc.)
- **ANY Shell Command**: `#mcp-shell-server` only - NO EXCEPTIONS

**Reason**: `run_in_terminal` has output reading bugs that cause incomplete results and development workflow failures.

### ğŸ§  Associative Memory Usage for Development Workflow

**Purpose**: Use MCP Associative Memory (`#mcp-mcp-assoc-mem`) as external memory augmentation for complex development projects.

**MANDATORY Usage Patterns:**
1. **Project State Persistence**: 
   - `memory_store` critical findings, architectural discoveries, implementation gaps
   - Store immediately after major technical discoveries or design decisions
   - Include context and rationale, not just raw facts

2. **Context Continuity**: 
   - `memory_search` before starting new analysis to avoid redundant work
   - Maintain knowledge across sessions and conversation boundaries
   - Build on previous discoveries rather than reanalyzing

3. **Discovery Tracking**:
   - Record important code findings, especially in large codebases
   - Track implementation status, component relationships
   - Update assessments when new discoveries change understanding

4. **Sprint & Project Management**:
   - Store sprint progress, issues, retrospective learnings
   - Track technical debt, architectural decisions
   - Preserve strategic direction and priority rationale

**Memory Organization Strategy:**
```
work/projects/llmcmd/     # Project-specific knowledge
  â”œâ”€â”€ architecture/       # System design insights
  â”œâ”€â”€ implementation/     # Code structure findings  
  â”œâ”€â”€ sprint-management/  # Agile process tracking
  â””â”€â”€ technical-issues/   # Problems and solutions

workflow/                 # Cross-project methodologies
  â”œâ”€â”€ memory-usage/       # Meta-usage patterns
  â”œâ”€â”€ development/        # General dev practices
  â””â”€â”€ tools/             # Tool-specific learnings
```

**Integration Rules:**
- **Search First**: Always `memory_search` relevant topics before deep analysis
- **Store Immediately**: Use `memory_store` after significant findings or decisions
- **Update When Changed**: Correct previous assessments with new discoveries
- **Use Descriptive Categories**: Enable future searchability with clear tags
- **Include Context**: Store not just what, but why and how decisions were made

**Benefits:**
- Prevents redundant analysis of large codebases
- Maintains project knowledge across development sessions  
- Enables faster onboarding and context switching
- Creates searchable project knowledge base
- Supports complex, multi-session development workflows

**Example Workflow:**
```
1. memory_search "VFS implementation llmcmd" 
2. Analyze findings, build on existing knowledge
3. memory_store new discoveries with context
4. Continue development with enhanced understanding
```

This creates persistent, searchable knowledge that augments GitHub Copilot's capabilities for complex, long-term development projects.

## Implementation Phases

### Phase 1: Foundation (Days 1-3)
- [x] Project structure and documentation
- [x] Go module initialization  
- [x] Basic CLI argument parsing
- [x] Configuration file loading
- [x] Logging infrastructure

### Phase 2: OpenAI Integration (Days 4-7)
- [x] HTTP client implementation
- [x] OpenAI API type definitions
- [x] Authentication and error handling
- [x] Response parsing
- [x] Retry mechanisms

### Phase 3: Tool Implementation (Days 8-13)
- [x] read tool: File/stream reading with fd management
- [x] write tool: File/stream writing with size tracking
- [x] spawn tool: Background-only command execution and data transfer
- [x] exit tool: Program termination with cleanup

### Phase 4: Built-in Commands (Days 14-17)
- [x] cat: Data copying
- [x] grep: Pattern matching (basic regex)
- [x] sed: Text substitution (basic functionality)
- [x] head/tail: Line-based filtering
- [x] sort: Alphabetical sorting
- [x] wc: Counting (lines, words, characters)
- [x] tr: Character translation

### Phase 5: Integration & Testing (Days 18-22) - COMPLETED
- [x] Main application logic
- [x] Tool orchestration
- [x] Comprehensive error handling
- [x] Security feature integration
- [x] Performance optimization

### Phase 6: Advanced Features (v3.0.0) - COMPLETED
- [x] Complete Quota System with weighted tokens
- [x] Fail-First configuration validation
- [x] API call limits and enforcement
- [x] Enhanced error handling architecture
- [x] Real-time quota tracking and display

### Additional Enhancement: Preset Prompt System
- [x] Stage 1: CLI extension with --preset/-r flags
- [x] Stage 2: Configuration file extension with preset definitions
- [x] Stage 3: Preset resolution in application initialization
- [x] Built-in diff/patch commands for technical operations

## Project Structure

```
llmcmd/
â”œâ”€â”€ cmd/llmcmd/           # Entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ app/              # Application logic
â”‚   â”œâ”€â”€ cli/              # CLI parsing and configuration
â”‚   â”œâ”€â”€ openai/           # OpenAI API client
â”‚   â”œâ”€â”€ tools/            # Tool execution engine
â”‚   â”‚   â””â”€â”€ builtin/      # Built-in command implementations
â”‚   â””â”€â”€ security/         # Security controls
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ examples/             # Usage examples
â””â”€â”€ README.md
```

## Security Guidelines

### Built-in Command Security
- No file system access beyond specified files
- No shell metacharacter interpretation
- No environment variable access
- Stream-based processing only

### API Cost Controls
- Configurable API call limits per session
- 300-second execution timeout
- Quota system with weighted token tracking (Input:1.0, Cached:0.25, Output:4.0)
- Real-time quota monitoring and enforcement
- 4KB read buffer limit
- 10MB file size limits

### Error Handling
- Graceful degradation
- Detailed error logging
- Secure error messages (no sensitive data exposure)

## Testing Strategy

### Test Pyramid
- **Unit Tests**: Individual function validation (high coverage)
- **Integration Tests**: API and tool interaction testing
- **E2E Tests**: Real-world scenario validation

### Mock Strategy
- OpenAI API mocking for reproducible tests
- File system mocking for I/O testing
- Command execution simulation

## Deliverables

### Version 1.0.0 Release
- [ ] Executable binary `llmcmd`
- [ ] Configuration template `.llmcmdrc`
- [ ] Complete documentation suite
- [ ] Comprehensive test suite
- [ ] Cross-platform binaries (Linux, macOS, Windows)

### Quality Requirements
- Unit test coverage â‰¥ 80%
- All major E2E scenarios tested
- Memory leak free
- Complete documentation
- Security audit passed

## Development Best Practices

### Code Quality
- Follow Go conventions and idioms
- Use standard library when possible
- Minimize external dependencies
- Clear, descriptive naming

### Git Workflow
- Feature branches for new functionality
- Descriptive commit messages
- Regular commits with logical groupings
- Proper branch management

### Performance Targets
- Startup time: < 100ms
- Memory usage: < 50MB baseline
- API response time: OpenAI API + minimal processing overhead
- File processing: Streaming for large files

This structured approach ensures secure, maintainable, and efficient development of the llmcmd tool.
